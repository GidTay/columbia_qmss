{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4: Text Models & Neural Networks\n",
    "Submitted by: Gideon Tay\\\n",
    "My UNI: gt2528\\\n",
    "Contact me at: gideon.tay@columbia.edu\n",
    "\n",
    "## Part A: Build a classification model using text data\n",
    "\n",
    "Let's first import all the required libraries for Part A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries required for Part A\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1. Import the data. The headlines will become your vectorized X matrix, and the labels indicate a binary classification (clickbait or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MyBook Disk Drive Handles Lots of Easy Backups</td>\n",
       "      <td>not clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CIT Posts Eighth Loss in a Row</td>\n",
       "      <td>not clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candy Carson Singing The \"National Anthem\" Is ...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why You Need To Stop What You're Doing And Dat...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27 Times Adele Proved She's Actually The Reale...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline          label\n",
       "0     MyBook Disk Drive Handles Lots of Easy Backups  not clickbait\n",
       "1                     CIT Posts Eighth Loss in a Row  not clickbait\n",
       "2  Candy Carson Singing The \"National Anthem\" Is ...      clickbait\n",
       "3  Why You Need To Stop What You're Doing And Dat...      clickbait\n",
       "4  27 Times Adele Proved She's Actually The Reale...      clickbait"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data from local directory\n",
    "df = pd.read_csv('text_training_data.csv')\n",
    "\n",
    "# Display the first 5 rows of the data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Convert the headline data into an X feature matrix using a simple bag of words approach.\n",
    "\n",
    "We convert the `label` column to a binary variable where 1 is clickbait, and 0 is not clickbait. Then, we use `CountVectorizer()` to apply a simple bag of words approach to transform the data in `headline` into an X feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (24979, 20332)\n",
      "Number of headlines: 24979\n",
      "Number of features: 20332\n",
      "The sparse matrix encoding:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n",
      "Every 1000th feature:\n",
      "['00' 'anaconda' 'begins' 'camera' 'compostela' 'deliver' 'electroputere'\n",
      " 'flats' 'gromit' 'ignores' 'kershner' 'mainframes' 'movement' 'overwatch'\n",
      " 'predators' 'relieved' 'screening' 'spadafora' 'tartan' 'undecided'\n",
      " 'wizard']\n"
     ]
    }
   ],
   "source": [
    "# Extract the 'headline' and 'label' columns\n",
    "headlines = df['headline']\n",
    "labels = df['label']\n",
    "\n",
    "# Convert labels to binary (1 for clickbait, 0 for not clickbait)\n",
    "y = labels.map({'clickbait': 1, 'not clickbait': 0})\n",
    "\n",
    "# Vectorize the headline data using a Bag-of-Words approach\n",
    "vect = CountVectorizer()\n",
    "X = vect.fit_transform(headlines)\n",
    "\n",
    "# Explore the resulting matrix\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of headlines: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"The sparse matrix encoding:\\n{X.toarray()}\")\n",
    "feature_names = vect.get_feature_names_out()\n",
    "print(f\"Every 1000th feature:\\n{feature_names[::1000]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X feature matrix shape suggests that there are 24979 headlines and 20332 features, with each feature corresponding to a unique word. \n",
    "\n",
    "The array shows the number of times each unique word appears in a given headline. Naturally, most of the values would be 0 (thus the name 'sparse matrix encoding'), since each headline would only have a small amount of words compared to the 20332 unique words in our feature set.\n",
    "\n",
    "I have also printed out every 1000th feature out of the 20332 features, to give you a small sample of features (unique words) in our feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Run logistic regression to predict clickbait headlines. Remember to train_test_split your data and use GridSearchCV to find the best value of C. You should evaluate your data with F1 scoring.\n",
    "\n",
    "Let's first split the data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (features): (19983, 20332)\n",
      "Training data shape (target): (19983,)\n",
      "Testing data shape (features): (4996, 20332)\n",
      "Testing data shape (target): (4996,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# Confirm the split sizes\n",
    "print(f\"Training data shape (features): {X_train.shape}\")\n",
    "print(f\"Training data shape (target): {y_train.shape}\")\n",
    "print(f\"Testing data shape (features): {X_test.shape}\")\n",
    "print(f\"Testing data shape (target): {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set up a logistic regresssion model and use `GridSearchCV()` to find the best value for parameter `C`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.97\n",
      "Best value for C: 100\n"
     ]
    }
   ],
   "source": [
    "# Set up the logistic regression model and GridSearchCV\n",
    "log_reg = LogisticRegression(solver='liblinear')  # Use liblinear for small datasets\n",
    "params = {'C': [0.01, 0.1, 1, 10, 100]}  # Values for regularization strength\n",
    "\n",
    "# Use F1 score for GridSearchCV, statified 5-fold cross validation\n",
    "grid = GridSearchCV(log_reg, params, cv=5, scoring='f1')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameter and cross-validation score\n",
    "print(f\"Best cross-validation score: {round(grid.best_score_,2)}\")\n",
    "print(f\"Best value for C: {grid.best_params_['C']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, evaluate the logistic regression model that uses the best parameter value, using an f1 score and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on the test set: 0.97\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Clickbait       0.97      0.97      0.97      2610\n",
      "    Clickbait       0.97      0.96      0.97      2386\n",
      "\n",
      "     accuracy                           0.97      4996\n",
      "    macro avg       0.97      0.97      0.97      4996\n",
      " weighted avg       0.97      0.97      0.97      4996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score on the test set: {round(f1,2)}\")\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, y_pred, target_names=['Not Clickbait', 'Clickbait']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4. Run 2 more logistic regression models by changing the vectorization approach (e.g. using n-grams, stop_words, and other techniques we discussed). In both cases, keep your logistic regression step the same. Only change how you're generating the X matrix from the text data.\n",
    "\n",
    "**Model 2: Unigram + Bigram**\n",
    "\n",
    "For our first vectorization approach, let us consider both unigrams and bigrams. This means that we consider sets of 2 words as features as well, beyond single words. This could be useful since sometimes words' meanings are only derived not alone but in pairs. As you can see below, our feature set increases greatly, from 20,332 to 135,950, and there are both single and double word features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (24979, 135950)\n",
      "Number of features: 135950\n",
      "Every 5000th feature:\n",
      "['00' 'about talking' 'and israel' 'award belonging' 'boris'\n",
      " 'celtics keep' 'copies' 'diet' 'especially for' 'fluffy puppies'\n",
      " 'gets 10' 'held in' 'in monster' 'jenner received' 'life hacks'\n",
      " 'men shirt' 'new indictments' 'on debt' 'permitted' 'purchase mobile'\n",
      " 'rights victory' 'sharp decline' 'steak and' 'that made' 'to defeat'\n",
      " 'uninjured after' 'when eddie' 'young indians']\n"
     ]
    }
   ],
   "source": [
    "# New Approach: Unigram + Bigram\n",
    "vect2 = CountVectorizer(ngram_range=(1, 2))\n",
    "X2 = vect2.fit_transform(headlines)\n",
    "\n",
    "# Explore the resulting matrix\n",
    "print(f\"Feature matrix shape: {X2.shape}\")\n",
    "print(f\"Number of features: {X2.shape[1]}\")\n",
    "feature_names2 = vect2.get_feature_names_out()\n",
    "print(f\"Every 5000th feature:\\n{feature_names2[::5000]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate the model. Since we will have to repeat this process again for the next model, we define a function so we can re-use it later,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.972\n",
      "Best value for C: 100\n",
      "F1 Score on the test set: 0.968\n"
     ]
    }
   ],
   "source": [
    "def split_gridSearch_evaluate(X, y):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "    \n",
    "    # Use GridSearchCV to find the best parameter\n",
    "    grid = GridSearchCV(log_reg, params, cv=5, scoring='f1')\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Get best parameter and cross-validation score\n",
    "    best_cross_val_score = round(grid.best_score_,3)\n",
    "    best_C = grid.best_params_['C']\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    f1 = round(f1_score(y_test, y_pred),3)\n",
    "\n",
    "    # Return key values\n",
    "    return {\n",
    "        \"best_cross_val_score\": best_cross_val_score,\n",
    "        \"best_C\": best_C,\n",
    "        \"f1_score\": f1,\n",
    "        \"best_model\": best_model\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "results2 = split_gridSearch_evaluate(X2, y)\n",
    "print(f\"Best cross-validation score: {results2[\"best_cross_val_score\"]}\")\n",
    "print(f\"Best value for C: {results2[\"best_C\"]}\")\n",
    "print(f\"F1 Score on the test set: {results2[\"f1_score\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 3: remove infrequent + stop words**\n",
    "\n",
    "For our second new vectorization approach, remove `stop_words` and words that appear in less than 2 headlines. Stop words are words that do not have much meaning and are thus not useful to aid in prediction. Removing `stop_words` and infrequent words reduces our feature set from 20,332 to 10,984."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (24979, 10984)\n",
      "Number of features: 10984\n",
      "Every 1000th feature:\n",
      "['00' 'base' 'climbing' 'doc' 'fred' 'inexpensive' 'mainstream'\n",
      " 'overturns' 'recruit' 'sirius' 'toddlers']\n"
     ]
    }
   ],
   "source": [
    "# New Approach: Remove infrequent and stop words\n",
    "vect3 = CountVectorizer(min_df=2, stop_words=\"english\")\n",
    "X3 = vect3.fit_transform(headlines)\n",
    "\n",
    "# Explore the resulting matrix\n",
    "print(f\"Feature matrix shape: {X3.shape}\")\n",
    "print(f\"Number of features: {X3.shape[1]}\")\n",
    "feature_names3 = vect3.get_feature_names_out()\n",
    "print(f\"Every 1000th feature:\\n{feature_names3[::1000]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate the model, using the `split_gridSearch_evaluate()` function we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.948\n",
      "Best value for C: 10\n",
      "F1 Score on the test set: 0.945\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "results3 = split_gridSearch_evaluate(X3, y)\n",
    "print(f\"Best cross-validation score: {results3[\"best_cross_val_score\"]}\")\n",
    "print(f\"Best value for C: {results3[\"best_C\"]}\")\n",
    "print(f\"F1 Score on the test set: {results3[\"f1_score\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A5. Which of your 3 models performed best? What are the most significant coefficients in each, and how do they compare?\n",
    "\n",
    "Let's summarize the F1 scores each model has against the test set in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Description</th>\n",
       "      <th>Test set F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model 1</td>\n",
       "      <td>simple bag-of-words</td>\n",
       "      <td>0.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model 2</td>\n",
       "      <td>unigram + bigram</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model 3</td>\n",
       "      <td>remove infrequent + stop words</td>\n",
       "      <td>0.945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model                     Description  Test set F1 score\n",
       "0  Model 1             simple bag-of-words              0.966\n",
       "1  Model 2                unigram + bigram              0.968\n",
       "2  Model 3  remove infrequent + stop words              0.945"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary for the DataFrame\n",
    "f1_score_results = {\n",
    "    'Model': ['Model 1', 'Model 2', 'Model 3'],\n",
    "    'Description': [\n",
    "        'simple bag-of-words', \n",
    "        'unigram + bigram', \n",
    "        'remove infrequent + stop words'\n",
    "        ],\n",
    "    'Test set F1 score': [round(f1,3), results2[\"f1_score\"], results3[\"f1_score\"]]\n",
    "}\n",
    "\n",
    "# Create and display the DataFrame\n",
    "f1_score_results_table = pd.DataFrame(f1_score_results)\n",
    "f1_score_results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 and 2 have similar performance, though model 2 which also considers bigrams beyond unigrams performs marginally better. Model 3 is the worst-performing model with the lowest F1 score.\n",
    "\n",
    "Now let's observe the most significant coefficients in each model. For each model, we only consider the one with the best parameter `C` value based on `GridSearchCV` conducted earlier. Then, we extract the top 10 coefficients with the highest absolute value, along with their corresponding feature names, and display them in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model 1 Feature</th>\n",
       "      <th>Model 1 Coefficient</th>\n",
       "      <th>Model 2 Feature</th>\n",
       "      <th>Model 2 Coefficient</th>\n",
       "      <th>Model 3 Feature</th>\n",
       "      <th>Model 3 Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ferry</td>\n",
       "      <td>-9.294036</td>\n",
       "      <td>you</td>\n",
       "      <td>6.392502</td>\n",
       "      <td>guess</td>\n",
       "      <td>5.343873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pitch</td>\n",
       "      <td>-8.514286</td>\n",
       "      <td>your</td>\n",
       "      <td>6.300404</td>\n",
       "      <td>kills</td>\n",
       "      <td>-4.484662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you</td>\n",
       "      <td>7.289243</td>\n",
       "      <td>confessions</td>\n",
       "      <td>5.858850</td>\n",
       "      <td>2015</td>\n",
       "      <td>4.430760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>7.047258</td>\n",
       "      <td>this</td>\n",
       "      <td>5.836613</td>\n",
       "      <td>crossword</td>\n",
       "      <td>4.372021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>6.966647</td>\n",
       "      <td>these</td>\n",
       "      <td>5.467637</td>\n",
       "      <td>know</td>\n",
       "      <td>4.319080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>victories</td>\n",
       "      <td>6.586090</td>\n",
       "      <td>but not</td>\n",
       "      <td>5.417677</td>\n",
       "      <td>zealand</td>\n",
       "      <td>-4.258099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>confessions</td>\n",
       "      <td>6.539295</td>\n",
       "      <td>21</td>\n",
       "      <td>5.109011</td>\n",
       "      <td>guy</td>\n",
       "      <td>4.244425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>these</td>\n",
       "      <td>6.431714</td>\n",
       "      <td>2015</td>\n",
       "      <td>4.953683</td>\n",
       "      <td>sucked</td>\n",
       "      <td>4.243858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>your</td>\n",
       "      <td>6.368054</td>\n",
       "      <td>17</td>\n",
       "      <td>4.935882</td>\n",
       "      <td>inauguration</td>\n",
       "      <td>-4.140832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>feminist</td>\n",
       "      <td>6.191835</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>4.804098</td>\n",
       "      <td>character</td>\n",
       "      <td>4.099228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model 1 Feature  Model 1 Coefficient Model 2 Feature  Model 2 Coefficient  \\\n",
       "0           ferry            -9.294036             you             6.392502   \n",
       "1           pitch            -8.514286            your             6.300404   \n",
       "2             you             7.289243     confessions             5.858850   \n",
       "3            2015             7.047258            this             5.836613   \n",
       "4        buzzfeed             6.966647           these             5.467637   \n",
       "5       victories             6.586090         but not             5.417677   \n",
       "6     confessions             6.539295              21             5.109011   \n",
       "7           these             6.431714            2015             4.953683   \n",
       "8            your             6.368054              17             4.935882   \n",
       "9        feminist             6.191835        buzzfeed             4.804098   \n",
       "\n",
       "  Model 3 Feature  Model 3 Coefficient  \n",
       "0           guess             5.343873  \n",
       "1           kills            -4.484662  \n",
       "2            2015             4.430760  \n",
       "3       crossword             4.372021  \n",
       "4            know             4.319080  \n",
       "5         zealand            -4.258099  \n",
       "6             guy             4.244425  \n",
       "7          sucked             4.243858  \n",
       "8    inauguration            -4.140832  \n",
       "9       character             4.099228  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to extract top 10 coefficients from a model and vectorizer\n",
    "def get_top_coefficients(best_model, vect):\n",
    "    coefficients = best_model.coef_.flatten()\n",
    "    feature_names = vect.get_feature_names_out()\n",
    "\n",
    "    # Pair coefficients with feature names and sort\n",
    "    coef_with_terms = sorted(\n",
    "        zip(feature_names, coefficients), # Create list of tuples of (coeff, feature)\n",
    "        key=lambda x: abs(x[1]), # Sort by absolute values of coefficients\n",
    "        reverse=True # So arranged from highest to lowest\n",
    "        )\n",
    "    \n",
    "    # Return top 10 coefficients and feature names only\n",
    "    top_10 = coef_with_terms[:10]\n",
    "    df = pd.DataFrame(top_10, columns=['Feature', 'Coefficient'])\n",
    "    return df\n",
    "\n",
    "# Extract top 10 coefficients for each model and store in DataFrames\n",
    "top_coeffs_model1 = get_top_coefficients(best_model, vect)\n",
    "top_coeffs_model2 = get_top_coefficients(results2['best_model'], vect2)\n",
    "top_coeffs_model3 = get_top_coefficients(results3['best_model'], vect3)\n",
    "\n",
    "# Rename the 'Feature' and 'Coefficient' columns for each model\n",
    "top_coeffs_model1 = top_coeffs_model1.rename(\n",
    "    columns={'Feature': 'Model 1 Feature', 'Coefficient': 'Model 1 Coefficient'})\n",
    "top_coeffs_model2 = top_coeffs_model2.rename(\n",
    "    columns={'Feature': 'Model 2 Feature', 'Coefficient': 'Model 2 Coefficient'})\n",
    "top_coeffs_model3 = top_coeffs_model3.rename(\n",
    "    columns={'Feature': 'Model 3 Feature', 'Coefficient': 'Model 3 Coefficient'})\n",
    "\n",
    "# Merge the DataFrames side by side and display them\n",
    "df_comparison = pd.concat([\n",
    "    top_coeffs_model1,\n",
    "    top_coeffs_model2,\n",
    "    top_coeffs_model3\n",
    "], axis=1)\n",
    "df_comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations: \n",
    "- Across all 3 models, the most significant coefficients have absolute values between 4 and 10.\n",
    "\n",
    "- Model 2's features with the top coefficients are 'you' and 'your', and these two words appear in Model 1's top 10 list as well. However, 'you' and 'your' may be stop words removed in model 3. Perhaps, such words have good predictive value of whether a headline is clickbait, as clickbait headlines may tend to address the reader directly. Then, the traditional stop word list may not be most appropriate. Indeed, we previously observed that model 3 has the lowest F1 score among the 3 models.\n",
    "\n",
    "- Model 2 has a bigram 'but not' in the top 10. Such a feature is not possible in the other 2 models, which don't allow for bigrams.\n",
    "\n",
    "- Interestingly, we only find some, but limited overlaps in the top predictive features across the 3 models, despite them using the same underlying data. This shows that the choice of the vectorization approach when producing feature sets from text data heavily affects the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Build a Predictive Neural Network Using Keras\n",
    "\n",
    "Let's first import all the libraries needed in Part B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries required for Part B (not yet imported in Part A)\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Import and load the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset from url\n",
    "url = \"http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Remove the 'rownames' column (no useful information)\n",
    "data = data.loc[:, data.columns != 'rownames'] \n",
    "\n",
    "# Display the first 5 rows\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Using the Sequential interface in Keras, build a model with 2 hidden layers with 16 neurons in each. Compile and fit the model. Assess its performance using accuracy on data that has been train_test_split.\n",
    "\n",
    "Let us first pre-process the data by splitting it into features (X) and target (y), one-hot encoding the target, and scaling the features. Then, split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 120\n",
      "Test samples: 30\n",
      "No. of features: 4\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "X = data.loc[:, data.columns != 'Species'] # Drop Species column\n",
    "y = data['Species'] # Target column is 'Species'\n",
    "\n",
    "# Encode species names into numeric labels to pass into to_categorical\n",
    "label_encoder = LabelEncoder()\n",
    "y_int = label_encoder.fit_transform(y)  # Converts to [0, 1, 2]\n",
    "\n",
    "# One-hot encode the y data using to_categorical(), 3 categories\n",
    "y_encoded = keras.utils.to_categorical(y_int, num_classes=3)\n",
    "\n",
    "# Standardize the features, may improve predictive accuracy\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# Print some information about our dataset\n",
    "print(f\"Training samples: {X_train.shape[0]}\") \n",
    "print(f\"Test samples: {X_test.shape[0]}\") \n",
    "print(f\"No. of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key variable of interest which we want to predict is `Species`. To better understand this classification problem, let us first observe all the possible unique values of `Species` in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# List unique values in 'Species'\n",
    "unique_values = data['Species'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only 3 possible species, our model's output layer will have 3 categories. Also note that we have 4 predictor variables: `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`.\n",
    "\n",
    "Below, we build a model with 4 inputs (our predictor variables), 2 hidden layers with 16 nodes each, and 1 output layer with 3 categories. We use the `relu` activation function for the hidden layers, and the `softmax` function for the output layer, which is used to calculate 0 to 1 probabilities for each of the 3 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gideo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">403</span> (1.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m403\u001b[0m (1.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">403</span> (1.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m403\u001b[0m (1.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build model using Sequential interface in Keras\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(4,)), # 4 inputs; 16 nodes in hidden layer 1\n",
    "    Activation('relu'), # relu for hidden layer 1\n",
    "    Dense(16), # 16 nodes in hidden layer 2\n",
    "    Activation('relu'), # relu for hidden layer 2\n",
    "    Dense(3), # 3 categories in output layer\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "# View the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built and specified the model's structure, we can now compile and fit the model. Compiling a model enables us to configure its learning process. Here, we use a stochastic gradient descent optimizer and specify loss as `categorical_crossentropy` since we have 3 possible categories of `Species`. After compiling, we fit our model with the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 513ms/step - accuracy: 0.2417 - loss: 1.3630\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2500 - loss: 1.3327\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2417 - loss: 1.3040\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2667 - loss: 1.2769\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2667 - loss: 1.2511\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2750 - loss: 1.2267\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2750 - loss: 1.2034\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2750 - loss: 1.1813\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3083 - loss: 1.1602\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3500 - loss: 1.1401\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3667 - loss: 1.1209\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3667 - loss: 1.1025\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3833 - loss: 1.0849\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4000 - loss: 1.0681\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4250 - loss: 1.0520\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4583 - loss: 1.0365\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4750 - loss: 1.0217\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 1.0075\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.9939\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5333 - loss: 0.9808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x296b4d102c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model: configure its learning process\n",
    "model.compile(loss='categorical_crossentropy', # Since there are 3 categories\n",
    "              optimizer='sgd', # Stochastic gradient descent\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can assess the model's performance on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.000\n",
      "Test Accuracy: 0.567\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss: {:.3f}\".format(score[0]))\n",
    "print(\"Test Accuracy: {:.3f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Run 2 additional models using different numbers of hidden layers and/or hidden neurons\n",
    "\n",
    "Let's build, compile, fit, and evaluate the performance of a model with 3 hidden layers instead of 2, while keeping the number of neurons per hidden layer (16) the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss for model 2: 0.886\n",
      "Test Accuracy for model 2: 0.867\n"
     ]
    }
   ],
   "source": [
    "# Build model using Sequential interface in Keras\n",
    "model2 = Sequential([\n",
    "    Dense(16, input_shape=(4,)), # 4 inputs; 16 nodes in hidden layer 1\n",
    "    Activation('relu'), # relu for hidden layer 1\n",
    "    Dense(16), # 16 nodes in hidden layer 2\n",
    "    Activation('relu'), # relu for hidden layer 2\n",
    "    Dense(16), # 16 nodes in hidden layer 3\n",
    "    Activation('relu'), # relu for hidden layer 3\n",
    "    Dense(3), # 3 categories in output layer\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "# Compile the model: configure its learning process\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model2.fit(X_train, y_train, epochs=20, batch_size=128, verbose=0)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "score2 = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss for model 2: {:.3f}\".format(score2[0]))\n",
    "print(\"Test Accuracy for model 2: {:.3f}\".format(score2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build, compile, fit, and evaluate the performance of a model with 32 neurons per hidden layer instead of 16, while keeping the number of hidden layers (2) the same as our initial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss for model 3: 1.112\n",
      "Test Accuracy for model 3: 0.233\n"
     ]
    }
   ],
   "source": [
    "# Build model using Sequential interface in Keras\n",
    "model3 = Sequential([\n",
    "    Dense(32, input_shape=(4,)), \n",
    "    Activation('relu'), \n",
    "    Dense(32), \n",
    "    Activation('relu'), \n",
    "    Dense(3), \n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "# Compile the model: configure its learning process\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model3.fit(X_train, y_train, epochs=20, batch_size=128, verbose=0)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "score3 = model3.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss for model 3: {:.3f}\".format(score3[0]))\n",
    "print(\"Test Accuracy for model 3: {:.3f}\".format(score3[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B4. How does the performance compare between your 3 models?\n",
    "\n",
    "Let's summarize the test results (both loss and accuracy) for each of the 3 models in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Hidden layers</th>\n",
       "      <th>Hidden neurons per layer</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model 1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1.000244</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model 2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.886357</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model 3</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1.112339</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Hidden layers  Hidden neurons per layer      Loss  Accuracy\n",
       "0  Model 1              2                        16  1.000244  0.566667\n",
       "1  Model 2              3                        16  0.886357  0.866667\n",
       "2  Model 3              2                        32  1.112339  0.233333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary for the DataFrame\n",
    "test_result = {\n",
    "    'Model': ['Model 1', 'Model 2', 'Model 3'],\n",
    "    'Hidden layers': [2, 3, 2],\n",
    "    'Hidden neurons per layer': [16, 16, 32],\n",
    "    'Loss': [score[0], score2[0], score3[0]],\n",
    "    'Accuracy': [score[1], score2[1], score3[1]]\n",
    "}\n",
    "\n",
    "# Create and display the DataFrame\n",
    "df = pd.DataFrame(test_result)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy measures the proportion of correct predictions out of the total number of predictions, while loss quantifies the error for each prediction and aggregates it into a single number. Here, we use the categorical cross-entropy loss function.\n",
    "\n",
    "Model 2 has the highest accuracy and lowest loss among the 3 models.This suggests increasing the number of hidden layers improves model performance in our example.\n",
    "\n",
    "Model 3 has the lowest accuracy and highest loss among the 3 models. This suggests increasing the number of hidden neurons per layer does not improve model performance in our example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
